{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "* Reinforcement learning works by **trial and error** - aiming to maxmize a reward function/high score\n",
    "* It is **unsupervised**, and wanders around the problem space establishing structure to maximize its reward\n",
    "\n",
    "### Definitions\n",
    "* **Environment** - The problem space, like a video game or financial market\n",
    "* **State** - All relevant parameters defining the problem space\n",
    "* **Agent** - **All Elements** of the algorithm that interact with the state\n",
    "* **Action** - Choose action ${A_{i}}$ from the set of the actions\n",
    "* **Step** - Given the Agent's *action*, the *environment* is updated - this is one *step* \n",
    "* **Reward** - A reward or penalty is awarded based on the *Action* chosen by the agent (e.g. Points in a video game or P/L in finance)\n",
    "* **Target** - What the *Agent* tries to maximize\n",
    "* **Policy** - The *Deterministic* action the agent takes given the current state of the environment\n",
    "* **Episode** - a set of *Steps* taken until success is achieved or failure is observed. In finance this could be something like **Profit over the course of one year or bankruptcy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "* The **OpenAI Gym** environment allows for the training of RL agents\n",
    "* The classic problem in Reinforcement Learning is **CartPole**, where an agent learns to move a cart left or right to balance a pole in the cart\n",
    "* The state of the environment is describred in a four variable vector {Cart Position, Cart Velocity, Pole Angle, Pole Velocity}\n",
    "* The following code pull in the Gym and inspects the observation source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import plt\n",
    "import gym\n",
    "import tensorflow.compat.v1 as tf\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import deque\n",
    "plt.style.use('seaborn')\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "os.environ['PYTHONHASHSEED'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0') # Environment Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(4,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space # Cart Position, Cart Velocity, Pole Angle, Pole Velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.8  ,   -inf, -0.419,   -inf], dtype=float16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.low.astype(np.float16) # Minimum value in observation 4-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.8  ,   inf, 0.419,   inf], dtype=float16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.high.astype(np.float16) # Maximum value in observation 4-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset() # Problem State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0467, -0.0347,  0.0249, -0.0232])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state # Ready to start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action Space\n",
    "env.action_space # Left, Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n # Two possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample() # Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the action\n",
    "state, reward, done, info = env.step(a) # Take a step with action a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.046 ,  0.16  ,  0.0244, -0.3079]), 1.0, False, {})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, reward, done, info # New state of environment, reward value, Are we finished boolean?, additional info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuation\n",
    "* While `done == False`, we can continue the game\n",
    "* Success condition is either a number of steps being reached or a particular reward being reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0393, -0.0018,  0.0169,  0.0479])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compete(max_steps, max_score):\n",
    "    env.reset()\n",
    "    total_reward = 0.\n",
    "    for e in range(1, max_steps):\n",
    "        a = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(a)\n",
    "        total_reward += reward\n",
    "        print(f'step={e:2d} | state={state} | action={a} | reward={total_reward}')\n",
    "        if done and (e + 1) < 200:\n",
    "            print('*** FAILED ***')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 1 | state=[ 0.0476 -0.216   0.0394  0.3122] | action=0 | reward=1.0\n",
      "step= 2 | state=[ 0.0433 -0.0215  0.0457  0.0322] | action=1 | reward=2.0\n",
      "step= 3 | state=[ 0.0429 -0.2172  0.0463  0.3389] | action=0 | reward=3.0\n",
      "step= 4 | state=[ 0.0385 -0.413   0.0531  0.6458] | action=0 | reward=4.0\n",
      "step= 5 | state=[ 0.0303 -0.6088  0.066   0.9548] | action=0 | reward=5.0\n",
      "step= 6 | state=[ 0.0181 -0.8047  0.0851  1.2674] | action=0 | reward=6.0\n",
      "step= 7 | state=[ 0.002  -0.6108  0.1105  1.0026] | action=1 | reward=7.0\n",
      "step= 8 | state=[-0.0102 -0.4173  0.1305  0.7465] | action=1 | reward=8.0\n",
      "step= 9 | state=[-0.0186 -0.614   0.1454  1.0773] | action=0 | reward=9.0\n",
      "step=10 | state=[-0.0309 -0.421   0.167   0.8335] | action=1 | reward=10.0\n",
      "step=11 | state=[-0.0393 -0.2285  0.1837  0.5977] | action=1 | reward=11.0\n",
      "step=12 | state=[-0.0439 -0.0364  0.1956  0.368 ] | action=1 | reward=12.0\n",
      "step=13 | state=[-0.0446 -0.2337  0.203   0.7154] | action=0 | reward=13.0\n",
      "step=14 | state=[-0.0493 -0.0419  0.2173  0.4929] | action=1 | reward=14.0\n",
      "*** FAILED ***\n"
     ]
    }
   ],
   "source": [
    "compete(200, 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Monte Carlo Approach\n",
    "* The CartPole problem can be solved with a simpler Monte Carlo approach, using the standard normally distributed weights + dot product apporach\n",
    "* Define which policy to adopt based on a partition of the Monte Carlo output\n",
    "* Define a large number of weights based on Monte Carlo simulation and select optimal weights\n",
    "* Define what counts as \"solution\" - e.g. mean score 195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.7302, -0.4214, -0.591 , -0.5365])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.random.random(4) * 2 - 1\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0325,  0.0424, -0.0248,  0.0036])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.028903315517161035"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.dot(weights, state)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define policy based on bounding dot product\n",
    "if s < 0:\n",
    "    a = 0\n",
    "else:\n",
    "    a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, weights):\n",
    "    state = env.reset()\n",
    "    treward = 0\n",
    "    for _ in range(200):\n",
    "        s = np.dot(state, weights)\n",
    "        a=0 if s < 0 else 1\n",
    "        state, reward, done, info = env.step(a)\n",
    "        treward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return treward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_episode(env, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    env.seed(seed)\n",
    "    tf.random.set_random_seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds()\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATE | episode=1\n",
      "UPDATE | episode=2\n",
      "SUCCESS | episode13\n"
     ]
    }
   ],
   "source": [
    "bestreward = 0\n",
    "for e in range(1, num_episodes+1):\n",
    "    weights = np.random.rand(4) * 2 - 1\n",
    "    treward = run_episode(env, weights)\n",
    "    if treward > bestreward:\n",
    "        bestreward = treward\n",
    "        bestweights = weights\n",
    "        if treward == 200:\n",
    "            print(f'SUCCESS | episode{e}')\n",
    "            break\n",
    "        print(f'UPDATE | episode={e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.4282,  0.7048,  0.95  ,  0.7697])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = []\n",
    "for _ in range(100):\n",
    "    treward = run_episode(env, weights)\n",
    "    res.append(treward)\n",
    "res[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(res) / len(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Network Approach\n",
    "* This problem can also be thought of as a classification problem: **what is the optimal action (label)** given the weights of the state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNAgent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.max = 0\n",
    "        self.scores = list()\n",
    "        self.memory = list()\n",
    "        self.model = self._build_model() # Private \n",
    "    \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=4, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid')) # Classification layer\n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001))\n",
    "        return model\n",
    "    \n",
    "    def act(self, state):\n",
    "        if random.random() <= 0.5:\n",
    "            return env.action_space.sample()\n",
    "        action = self.model.predict_classes(state, batch_size=None)[0, 0]\n",
    "        return action\n",
    "    \n",
    "    def train_model(self, state, action):\n",
    "        self.model.fit(state, np.array([action]), epochs=1, verbose=False) # Fit model from state space to action\n",
    "        \n",
    "    def learn(self, episodes):\n",
    "        for e in range(1, episodes+1):\n",
    "            state = env.reset()\n",
    "            for _ in range(201):\n",
    "                state = np.reshape(state, [1, 4])\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                if done:\n",
    "                    score = _ + 1\n",
    "                    self.scores.append(score)\n",
    "                    self.max = max(score, self.max) # Was this run the best?\n",
    "                    print('episode: {:4d}/{} | score: {:3d} | max: {:3d}'\n",
    "                                   .format(e, episodes, score, self.max), end='\\r')\n",
    "                    break\n",
    "                self.memory.append((state, action))\n",
    "                self.train_model(state, action) # Update the policy (by training the neural net)\n",
    "                state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(100)\n",
    "agent = NNAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  500/500 | score:  11 | max:  57\r"
     ]
    }
   ],
   "source": [
    "episodes = 500\n",
    "agent.learn(episodes) # Performance of agent - doesn't even approach the reinforcement approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "* Q-Learning is a more sophisticated approach, taking into account **delayed rewards** rather than immediate rewards\n",
    "* This works by computing an **action-value** policy ${Q}$ that assigns every combination of a state and action a value. The higher the value, the better the action.\n",
    "* ${Q}$ is the **sum of the actions direct reward and the discounted value of the optimal action in the next state**, formally ${Q(S_{t}, A_{t}) = R_{t+1} + \\gamma max_{a} Q(S_{t+1}, a)}$ with ${S_{t}}$ state at time t and ${A_{t}}$ the action taken at time t. ${R_{t+1}}$ is the reward of action ${A}$, ${\\gamma \\in }$ is a discounting factor and ${max_{a}Q(S_{t+1}, a}$ is the reward of the optimal action at the next step.\n",
    "* ${Q}$ should generally be thought of as a function in continuous space - closed form solutions to an optimal ${Q}$ can likely not be derived, so we **approximate**\n",
    "* This is where neural networks come into play - the appoximation is an optimization problem.\n",
    "* Another critical element is **replay** - the ${QL}$-replays over experiences to update the policy action ${Q}$. An optimal ${QL}$ agent starts with pure exploration of the space, then decrease the exploration rate (${\\epsilon}$) until it reaches a minimum level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent:\n",
    "    def __init__(self, finish=False):\n",
    "        self.finish = finish\n",
    "        self.epsilon = 1.0 # Initial Exploration %\n",
    "        self.epsilon_min = 0.01 # Minimum Exploration %\n",
    "        self.epsilon_decay = 0.995 # Decay rate of Epsilon\n",
    "        self.gamma = 0.95 # Discount factor of t+1 optimal choice\n",
    "        self.batch_size = 32\n",
    "        self.max_treward = 0\n",
    "        self.averages = list()\n",
    "        self.memory = deque(maxlen=2000) # Deque for limited history\n",
    "        self.osn = env.observation_space.shape[0] # NN input layer\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.osn, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu')) # Hidden layer\n",
    "        model.add(Dense(env.action_space.n, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "        return model\n",
    "    \n",
    "    def act(self, state):\n",
    "        if random.random() <= self.epsilon: # This means explore\n",
    "            return env.action_space.sample()\n",
    "        action = self.model.predict(state) # What's return value of this?\n",
    "        return np.argmax(action)\n",
    "    \n",
    "    def replay(self):\n",
    "        batch = random.sample(self.memory, self.batch_size) # Take a random batch of history to replay\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            if not done:\n",
    "                reward += self.gamma * np.amax(self.model.predict(next_state)[0]) # Q-value for state-action pair\n",
    "                target = self.model.predict(state)\n",
    "                target[0, action] = reward\n",
    "                self.model.fit(state, target, epochs=1, verbose=False) # Update neural net for action value pairs\n",
    "        if self.epsilon > self.epsilon_min: # Decay exploration rate\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def learn(self, episodes):\n",
    "        trewards = []\n",
    "        for e in range(1, episodes+1):\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, self.osn]) # Reshape state into one row input dim columns\n",
    "            for _ in range(5000):\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, self.osn])\n",
    "                self.memory.append([state, action, reward, next_state, done]) # Memory _ x 6\n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    treward = _ + 1\n",
    "                    trewards.append(treward)\n",
    "                    av = sum(trewards[-25:]) / 25 # Average of everything so far\n",
    "                    self.averages.append(av)\n",
    "                    self.max_treward = max(self.max_treward, treward)\n",
    "                    templ = 'episode: {:4d}/{} | treward: {:4d} |'\n",
    "                    templ += 'av: {:5.1f} | max: {:4d}'\n",
    "                    print(templ.format(e, episodes, treward, av, self.max_treward), end='\\r')\n",
    "                    break\n",
    "                \n",
    "                if av > 195 and self.finish:\n",
    "                     break\n",
    "                    \n",
    "                if len(self.memory) > self.batch_size:\n",
    "                    self.replay() # Replay once able to\n",
    "                    \n",
    "    def test(self, episodes):\n",
    "        trewards = []\n",
    "        for e in range(1, episodes+1):\n",
    "            state = env.reset()\n",
    "            for _ in range(1001):\n",
    "                state = np.reshape(state, [1, self.osn])\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    treward = _ + 1\n",
    "                    trewards.append(treward)\n",
    "                    print('episode: {:4d}/{} | treward: {:4d}'.format(e, episodes, treward), end='\\r')\n",
    "                    break\n",
    "                    \n",
    "            return trewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of QL-Agent\n",
    "* This agent performs extremely well, though the performance improvement is not monotonic.\n",
    "* The below run extraordinarily slowly - I should ping Yves and understand this and what `av` should be defaulted to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(100)\n",
    "agent = DQLAgent(finish=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'av' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-e726a44de019>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-170-b55285ad734d>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, episodes)\u001b[0m\n\u001b[1;32m     61\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mav\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m195\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                      \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'av' referenced before assignment"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "agent.learn(episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finance Applications\n",
    "* First, a simple finance gym working on time series\n",
    "* Success is achieved when the agent successfully trades through all of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class observation_space:\n",
    "    def __init__(self, n):\n",
    "        self.shape = (n, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class action_space:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "    \n",
    "    def sample(self):\n",
    "        return random.randint(0, self.n - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinanceGym:\n",
    "    \n",
    "    url = '../../source/aiif_eikon_eod_data.csv'\n",
    "    def __init__(self, symbol, features):\n",
    "        self.symbol = symbol\n",
    "        self.features = features\n",
    "        self.observation_space = observation_space(4)\n",
    "        self.osn = self.observation_space.shape[0]\n",
    "        self.action_space = action_space(2) # Long, Short\n",
    "        self.min_accuracy = .50\n",
    "        self._get_data()\n",
    "        self._prepare_data()\n",
    "        \n",
    "    def _get_data(self):\n",
    "        self.raw = pd.read_csv(self.url, index_col=0, parse_dates=True).dropna()\n",
    "        \n",
    "    def _prepare_data(self):\n",
    "        self.data = pd.DataFrame(self.raw[self.symbol])\n",
    "        self.data['r'] = np.log(self.data / self.data.shift(1))\n",
    "        self.data.dropna(inplace=True)\n",
    "        self.data = (self.data - self.data.mean()) / self.data.std()\n",
    "        self.data['d'] = np.where(self.data['r'] > 0, 1, 0)\n",
    "        \n",
    "    def _get_state(self): # Select data defining state of market\n",
    "        return self.data[self.features].iloc[self.bar - self.osn: self.bar].values \n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.treward = 0\n",
    "        self.accuracy = 0\n",
    "        self.bar = self.osn\n",
    "        state = self.data[self.features].iloc[self.bar - self.osn: self.bar]\n",
    "        return state.values\n",
    "    \n",
    "    def step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
